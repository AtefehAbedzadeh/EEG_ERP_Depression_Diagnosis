# step 1
import os
import numpy as np
import networkx as nx
from sklearn.metrics import pairwise_distances
from scipy.io import loadmat
import h5py

def euclidean_distance(point1, point2):
    return np.linalg.norm(point1 - point2)

def load_hdf5_file(file_path):
    with h5py.File(file_path, 'r') as f:
        data = f['data'][:]
    return data


all_dir = "/content/gdrive/MyDrive/ERP_all_data"
file_list = os.listdir(all_dir)

# Definition of graphs
graphs = []
for _ in range(128):
    graphs.append(nx.Graph())

# Reading and adding data to graphs
for i, file1 in enumerate(file_list):
    for j, file2 in enumerate(file_list):
        if i < j:

            file1_path = os.path.join(all_dir, file1)
            file2_path = os.path.join(all_dir, file2)

            if file1.endswith('.h5'):
                data1 = load_hdf5_file(file1_path)
                data1 = data1.reshape(data1.shape[1], -1)
                data1 = data1[:128,:]
            elif file1.endswith('.npy'):
                data1 = np.load(file1_path)
                data1 = data1.reshape(data1.shape[1], -1)
                data1 = data1[:128,:]

            if file2.endswith('.h5'):
                data2 = load_hdf5_file(file2_path)
                data2 = data2.reshape(data2.shape[1], -1)
                data2 = data2[:128,:]
            elif file2.endswith('.npy'):
                data2 = np.load(file2_path)
                data2 = data2.reshape(data2.shape[1], -1)
                data2 = data2[:128,:]


            for k in range(128):
                distance = euclidean_distance(data1[k, :], data2[k, :])
                graphs[k].add_edge(file1, file2, weight=distance)

# step 2    
import numpy as np

# Hyperparameters
m = 0.5
o = 2

# Calculate the transition probabilities between nodes
def calculate_transition_probabilities(graph, m, o):
    transition_probabilities = {}

    for node in graph.nodes:
        neighbors = list(graph.neighbors(node))
        total_weight = sum(graph[node][neighbor]['weight'] for neighbor in neighbors)

        probabilities = {}
        for neighbor in neighbors:
            weight = graph[node][neighbor]['weight']
            d_ux = nx.shortest_path_length(graph, source=node, target=neighbor)

            if d_ux == 0:
                alpha = 1 / m
            elif d_ux == 1:
                alpha = 1
            elif d_ux == 2:
                alpha = 1 / o

            probability = alpha * (weight / total_weight)
            probabilities[neighbor] = probability

        transition_probabilities[node] = probabilities

    return transition_probabilities

# Calculate transition probabilities for each graph
transition_probabilities_graph = []
for i in range(128):
    transition_probabilities_graph.append(calculate_transition_probabilities(graphs[i], m, o))

# step 3    
import random

# Generate random walks for all nodes of fixed length
def generate_random_walks_all_nodes(graph, transition_probabilities, walk_length):
    random_walks = []

    for source_node in graph.nodes:
        for _ in range(10):
            walk = [source_node]
            current_node = source_node

            for _ in range(walk_length - 1):
                neighbors = list(graph.neighbors(current_node))
                probabilities = [transition_probabilities[current_node].get(neighbor, 0) for neighbor in neighbors]
                probabilities_sum = sum(probabilities)

                if probabilities_sum > 0:
                    probabilities = [prob / probabilities_sum for prob in probabilities]
                    next_node = random.choices(neighbors, probabilities)[0]
                    walk.append(next_node)
                    current_node = next_node
                else:
                    break

            random_walks.append(walk)

    return random_walks


walk_length = 100

# Generate random walks for each graph
random_walks = []
for i in range(128):
    random_walks.append(generate_random_walks_all_nodes(graphs[i], transition_probabilities_graph[i], walk_length))

#step 4
from gensim.models import Word2Vec

d = 128

# Definition of the Word2Vec model
models = []
for i in range(128):
    model = Word2Vec(sentences=random_walks[i], vector_size=d, window=2, min_count=0, sg=1)
    models.append(model)

# node_embeddings
node_embeddings = []
for i in range(128):
    node_embeddings.append(models[i].wv)    

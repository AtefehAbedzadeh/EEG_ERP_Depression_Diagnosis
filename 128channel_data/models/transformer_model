import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, Input, Flatten
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.callbacks import Callback
import matplotlib.pyplot as plt
import time


combined_features = np.concatenate((features_node2vec, features_AlexNet, features_TSCN), axis=1)
print(combined_features.shape)

X_train, X_test, y_train, y_test = train_test_split(combined_features, label, test_size=0.1, random_state=42)

X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))


class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential(
            [Dense(ff_dim, activation="relu"), Dense(embed_dim),]
        )
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

embed_dim = 64  # Dimensionality of input
num_heads = 8  # Number of attention heads
ff_dim = 128  # Hidden layer size in feed forward network

# Transformer
model = Sequential()
model.add(Input(shape=(1, X_train.shape[2])))
model.add(Dense(embed_dim, activation='relu'))  # Reduce dimensionality
model.add(TransformerBlock(embed_dim, num_heads, ff_dim))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))


class ReduceLearningRateOnPlateau(Callback):
    def __init__(self, factor=0.5, patience=2, min_lr=1e-4):
        super(ReduceLearningRateOnPlateau, self).__init__()
        self.factor = factor
        self.patience = patience
        self.min_lr = min_lr
        self.wait = 0
        self.best_loss = float('inf')

    def on_epoch_end(self, epoch, logs=None):
        current_loss = logs.get('val_loss')
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                new_lr = max(self.model.optimizer.learning_rate * self.factor, self.min_lr)
                self.model.optimizer.learning_rate = new_lr
                print(f'Reducing learning rate to {new_lr} due to no improvement in validation loss.')
                self.wait = 0

reduce_lr = ReduceLearningRateOnPlateau(factor=0.5, patience=2, min_lr=1e-4)

model.compile(optimizer=SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])
history = model.fit(X_train, y_train, epochs=83, validation_data=(X_test, y_test), callbacks=[reduce_lr])


num_test_samples = len(X_test)
start_time = time.time()
predictions = model.predict(X_test)
end_time = time.time()
execution_time = (end_time - start_time) / num_test_samples
print("Execution Time (Model Inference per sample):", execution_time, "seconds")

model.save('depression_transformer.h5')


train_loss = history.history['loss']
train_accuracy = history.history['accuracy']
val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

# Loss curve
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Accuracy curve
plt.plot(train_accuracy, label='Training Accuracy')
plt.plot(val_accuracy, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
